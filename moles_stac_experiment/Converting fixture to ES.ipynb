{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f06a64-ef21-4d59-b338-b1aa87bb5286",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyze of Moles DB fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c2ad0-a727-418f-bcd1-0e3458a2ac7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58d823b-d8ab-456a-b284-669864485430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "\n",
    "# loads json file to dictionary\n",
    "def json_to_dict(filename):\n",
    "    with gzip.open(filename, 'r') as f:\n",
    "        return json.loads(f.read().decode('utf-8'))\n",
    "\n",
    "# saves dictionary to json file\n",
    "def dict_to_json(data, filename):\n",
    "    with gzip.open(filename, 'w') as f:\n",
    "        f.write(json.dumps(data).encode('utf-8'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f6ebd-8a96-4e8a-a88a-40f64d670a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Flattening fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040fed69-a883-4e6d-a6ed-74cfc97682fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Refereanceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200a2034-f74b-4fa2-a0ff-6f06101348a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns model's name\n",
    "def get_model_name(obj):\n",
    "    return obj['model'].split('.')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b20b56-56e3-446a-994a-b13adb5bea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps shortcode to model name\n",
    "\n",
    "map = {\n",
    "    'acq': 'acquisition',\n",
    "    'cmppr': 'compositeprocess',\n",
    "    'coll': 'observationcollection',\n",
    "    'comp': 'computation',\n",
    "    'instr': 'instrument',\n",
    "    'mpop': 'mobileplatformoperation',\n",
    "    'ob': 'observation',\n",
    "    'plat': 'platform',\n",
    "    'proj': 'project',\n",
    "    'result': 'result',\n",
    "    'excit': 'externalcitation',\n",
    "    }\n",
    "def map_shortcode_to_model_name(short_code, full=False):\n",
    "    if full:\n",
    "        return f'cedamoles_app.{map[short_code]}'\n",
    "    return map[short_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca38a81-6086-4073-9653-e50b2fafc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dict of referenceable objects where keys are PKs and values are fields of those objects\n",
    "\n",
    "def get_referenceable_dict_from_list(ref_list):\n",
    "    output = dict()\n",
    "    for i in ref_list:\n",
    "        output[i['pk']] =  i['fields']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8334282-93e7-4a0e-8098-a0725fc924d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inserts UUIDs into fields of corresponding models \n",
    "\n",
    "def include_referenceable(data):\n",
    "    output = []\n",
    "    referenceable = [i for i in data if i['model'] == 'cedamoles_app.referenceable']\n",
    "    referenceable = {i['pk']: i['fields'] for i in referenceable}\n",
    "\n",
    "    data = [i for i in data if i['model'] != 'cedamoles_app.referenceable']\n",
    "    \n",
    "    for i in data:\n",
    "        pk = i['pk']\n",
    "        if pk in referenceable and i['model'] == map_shortcode_to_model_name(referenceable[pk]['short_code'] , True):\n",
    "            my_obj = i\n",
    "            my_obj['fields']['uuid'] = referenceable[pk]['uuid']\n",
    "            output.append(my_obj)\n",
    "        else:\n",
    "            output.append(i)\n",
    "            \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132075f-6a31-4727-a0d2-8e06663e5982",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inserting models into models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7733306-79e5-4be9-aa28-e4fdec61f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. if observation has a filed phenomena which refers to the phenomena model by its PK, that model will be inserted into the observation\n",
    "\n",
    "def include_simple_field(data, model_name, model_and_field_pairs_to_insert_model):\n",
    "    values = {i['pk']: i['fields'] for i in data if i['model'] == f'cedamoles_app.{model_name}'}\n",
    "    data = [i for i in data if i['model'] != f'cedamoles_app.{model_name}']\n",
    "\n",
    "    for i in data:\n",
    "        for m, f in model_and_field_pairs_to_insert_model:\n",
    "            if i['model'] == f'cedamoles_app.{m}' and f in i['fields'] and i['fields'][f]:\n",
    "                if isinstance(i['fields'][f], list):\n",
    "                    i['fields'][f] = [values[j] for j in i['fields'][f]]\n",
    "                else:\n",
    "                    i['fields'][f] = values[i['fields'][f]]\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d2b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. if relation between 2 models is done via foreign key (identifiers and observations) then fields from non-referenceable will be added to the referenceable\n",
    "\n",
    "def include_on_foreign_key(data, model_name, new_field_name=''):\n",
    "    output = []\n",
    "    ids = [i for i in data if i['model'] == f'cedamoles_app.{model_name}']\n",
    "\n",
    "    rel_field_name = 'relatedTo' if 'relatedTo' in ids[0]['fields'] else 'ob_ref' if 'ob_ref' in ids[0]['fields'] else 'relatedRecord'\n",
    "\n",
    "    ids_dict = dict()\n",
    "    for i in ids:\n",
    "        if i['fields'][rel_field_name] not in ids_dict:\n",
    "            ids_dict[i['fields'][rel_field_name]] = i['fields']\n",
    "        \n",
    "        elif not isinstance(ids_dict[i['fields'][rel_field_name]], list):\n",
    "            ids_dict[i['fields'][rel_field_name]] = [ids_dict[i['fields'][rel_field_name]], i['fields']]\n",
    "\n",
    "        else:\n",
    "            ids_dict[i['fields'][rel_field_name]].append(i['fields'])\n",
    "\n",
    "\n",
    "    data = [i for i in data if i['model'] != f'cedamoles_app.{model_name}']\n",
    "    for i in data:\n",
    "        if 'uuid' in i['fields'] and i['pk'] in ids_dict:\n",
    "            if isinstance(ids_dict[i['pk']], list):\n",
    "                fields = [{k: v for k, v in j.items() if k != rel_field_name} for j in ids_dict[i['pk']]]\n",
    "            else:\n",
    "                fields = {k: v for k, v in ids_dict[i['pk']].items() if k != rel_field_name}\n",
    "\n",
    "            i['fields'][new_field_name if new_field_name else model_name] = fields\n",
    "        \n",
    "        output.append(i)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2f0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this fields is unique \n",
    "\n",
    "def include_related_obs_info(data):\n",
    "    output = []\n",
    "    ids = [i for i in data if i['model'] == 'cedamoles_app.relatedobservationinfo']\n",
    "    ids = {i['fields']['objectObservation']: i['fields'] for i in ids}\n",
    "    data = [i for i in data if i['model'] != 'cedamoles_app.relatedobservationinfo']\n",
    "\n",
    "    for i in data:\n",
    "        if i['pk'] in ids:\n",
    "            fields = {k: v for k, v in ids[i['pk']].items() if k != 'objectObservation'}\n",
    "            i['fields']['relatedObservationInfo'] = fields\n",
    "        \n",
    "        output.append(i)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132075f-6a31-4727-a0d2-8e06663e5982",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating relations and removing PKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cab7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps PK to UUID\n",
    "\n",
    "my_fixture = json_to_dict('fixture2.json.gz')\n",
    "my_map = {i['pk']: i['fields']['uuid'] for i in my_fixture}\n",
    "\n",
    "def map_pk_to_uuid(pk):\n",
    "    try:\n",
    "        return my_map[pk]\n",
    "    except KeyError:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012fc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_pks(obj, field):\n",
    "    if not field and isinstance(obj, int):\n",
    "        return map_pk_to_uuid(obj)\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        return [replace_pks(i, field) for i in obj]\n",
    "\n",
    "    if not field or field[0] not in obj:\n",
    "        return obj\n",
    "    \n",
    "    obj[field[0]] = replace_pks(obj[field[0]], field[1:])\n",
    "\n",
    "    return obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188e0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces PHs with UUIDs\n",
    "\n",
    "list_of_fields = ['independentInstrument',\n",
    "                'mobilePlatformOperation',\n",
    "                'computationComponent',\n",
    "                'acquisitionComponent',\n",
    "                'subInstrument',\n",
    "                'platform_field',\n",
    "                'result_field',\n",
    "                'procedureAcquisition',\n",
    "                'procedureComputation',\n",
    "                'procedureCompositeProcess',\n",
    "                'projects',\n",
    "                'member',\n",
    "                'parentProject',\n",
    "                'observationCollection',\n",
    "                'relatedObservationInfo/subjectObservation',\n",
    "                'instrumentPlatformPair/instrument',\n",
    "                'instrumentPlatformPair/platform',\n",
    "                'note/commentator',\n",
    "                'note/relatedRecord',\n",
    "                'reviewNote/commentator',\n",
    "                'softwareReference',\n",
    "                'childPlatform',\n",
    "                'oldDataPath'\n",
    "                ]\n",
    "\n",
    "def swap_pks_to_uuids(data):\n",
    "    output = []\n",
    "    for i in data:\n",
    "        for f in list_of_fields:\n",
    "            f = f.split('/')\n",
    "            i['fields'] = replace_pks(i['fields'], f)\n",
    "        \n",
    "        output.append(i)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b3a1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange structure of records by removing PKs and bringing fields to the upper level\n",
    "\n",
    "def remove_pks(data):\n",
    "    output = []\n",
    "    for i in data:\n",
    "        model = i['model'].split('.')[1]\n",
    "        i = i['fields']\n",
    "        i['model'] = model\n",
    "        output.append(i)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e81dbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes specified field from dictionary recursively\n",
    "\n",
    "def remove_from_dict(data, field_path):\n",
    "    if data is None: \n",
    "        return\n",
    "        \n",
    "    field_path = field_path.split('/')\n",
    "\n",
    "    if field_path[0] not in data:\n",
    "        return data\n",
    "\n",
    "    if not field_path[1:]:\n",
    "        return {k: v for k, v in data.items() if k != field_path[0]}\n",
    "\n",
    "    data[field_path[0]] = remove_from_dict(data[field_path[0]], '/'.join(field_path[1:]))\n",
    "    \n",
    "    return data\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d79977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes specified field of one of the models from dictionary\n",
    "\n",
    "def remove_field_from_model(data, model_name, field_path):\n",
    "    output = []\n",
    " \n",
    "    for i in data:\n",
    "        try:\n",
    "            if i['model'] == model_name:\n",
    "                i = remove_from_dict(i, field_path)\n",
    "            output.append(i)\n",
    "        except:\n",
    "            print(f'Problem with path {field_path}. i = {i}')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d04a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes list of model-fieldpath pairs, removes them from the fixture and saves result to the new one\n",
    "\n",
    "def remove_fields_from_models_in_fixture(list_of_model_fieldpath_pairs, file_in = 'fixture3.json.gz', file_out = 'fixture4.json.gz'):\n",
    "    data = json_to_dict(file_in)\n",
    "    \n",
    "    for m, f in list_of_model_fieldpath_pairs:\n",
    "        data = remove_field_from_model(data, m, f)\n",
    "        \n",
    "\n",
    "    dict_to_json(data, file_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c767",
   "metadata": {},
   "source": [
    "### Analyzing non empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13869e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_not_nulls_helper(path, obj, out_dict):\n",
    "    if (obj != 0 and not obj) or obj is None or obj == ' ':\n",
    "        out_dict[path] = out_dict.get(path, 0)\n",
    "        return\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        for i in obj:\n",
    "            # out_dict[path] = out_dict.get(path, 0) + 1\n",
    "            count_not_nulls_helper(path, i, out_dict)\n",
    "        return\n",
    "        \n",
    "    out_dict[path] = out_dict.get(path, 0) + 1\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            count_not_nulls_helper(f'{path}/{k}', v, out_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "019f24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts how many non empty/non None values of certain field appeared in the database\n",
    "\n",
    "def count_not_nulls(grouped_data, group_by_model = True):\n",
    "    output = dict()\n",
    "\n",
    "    for m, l in grouped_data.items():\n",
    "        for f in l:\n",
    "            if group_by_model:\n",
    "                count_not_nulls_helper(m, f, output)\n",
    "            else:\n",
    "                count_not_nulls_helper('', f, output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32d8bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups records by model\n",
    "\n",
    "def group_by_model(data):\n",
    "    output = dict()\n",
    "    models = set([i['model'] for i in data])\n",
    "\n",
    "    for m in models:\n",
    "        output[m] = []\n",
    "    \n",
    "    for i in data:\n",
    "        output[i['model']].append(i)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b406ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves distribution of values to the txt file\n",
    "\n",
    "def save_distribution_to_the_file(data, filename='distribution.txt', group_by_count = False, group_models = True):\n",
    "    \n",
    "    dicted_data = group_by_model(data)\n",
    "    distribution = count_not_nulls(dicted_data, group_models)\n",
    "\n",
    "    if not group_by_count:\n",
    "        with open(filename, 'w') as f:\n",
    "            k1 = ''\n",
    "            for k, v in distribution.items():\n",
    "                model = k.split('/')[0]\n",
    "\n",
    "                if k1 != model:\n",
    "                    f.write('\\n')\n",
    "\n",
    "                k1 = model\n",
    "                f.write(f'{k}: {v}\\n')\n",
    "        return\n",
    "\n",
    "    \n",
    "    result = dict()\n",
    "\n",
    "    for k, v in distribution.items():\n",
    "        if v in result:\n",
    "            result[v].append(k)\n",
    "        else:\n",
    "            result[v] = [k]\n",
    "    \n",
    "    result = dict(sorted(result.items()))\n",
    "\n",
    "    filename = filename.split('.')[0]\n",
    "    with open(f'{filename}_grouped.txt', 'w') as f:\n",
    "        for k,v in result.items():\n",
    "            f.write(f'{k}:\\n')\n",
    "            for i in v:\n",
    "                f.write(f'\\t{i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dabce",
   "metadata": {},
   "source": [
    "### Removing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f396661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes specified field from dictionary recursively\n",
    "\n",
    "def remove_from_dict(data, field_path):\n",
    "    if data is None: \n",
    "        return\n",
    "        \n",
    "    field_path = field_path.split('/')\n",
    "\n",
    "    if field_path[0] not in data:\n",
    "        return data\n",
    "\n",
    "    if not field_path[1:]:\n",
    "        return {k: v for k, v in data.items() if k != field_path[0]}\n",
    "\n",
    "    data[field_path[0]] = remove_from_dict(data[field_path[0]], '/'.join(field_path[1:]))\n",
    "    \n",
    "    return data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98306d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes specified field of one of the models from dictionary\n",
    "\n",
    "def remove_field_from_model(data, model_name, field_path):\n",
    "    output = []\n",
    " \n",
    "    for i in data:\n",
    "        try:\n",
    "            if i['model'] == model_name:\n",
    "                i = remove_from_dict(i, field_path)\n",
    "            output.append(i)\n",
    "        except:\n",
    "            print(f'Problem with path {field_path}. i = {i}')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08b50742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes list of model-fieldpath pairs, removes them from the fixture and saves result to the new one\n",
    "\n",
    "def remove_fields_from_models_in_fixture(list_of_model_fieldpath_pairs, file_in = 'fixture3.json.gz', file_out = 'fixture4.json.gz'):\n",
    "    data = json_to_dict(file_in)\n",
    "    \n",
    "    for m, f in list_of_model_fieldpath_pairs:\n",
    "        data = remove_field_from_model(data, m, f)\n",
    "        \n",
    "\n",
    "    dict_to_json(data, file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95131df2",
   "metadata": {},
   "source": [
    "### Saving structure of fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7951a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress all the records under model to one record with non empty fields (if possible) which will be representative\n",
    "\n",
    "def compress_groups(data_by_model):\n",
    "    output = dict()\n",
    "    for m, l in data_by_model.items():\n",
    "        output[m] = dict()\n",
    "        for i in l:\n",
    "            for k, v in i.items():\n",
    "                if k not in output[m] or not output[m][k]:\n",
    "                    output[m][k] = v\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0510af60-e2f6-4f8b-b28e-e2ab6c83736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert values to the type name\n",
    "\n",
    "def convert_values_to_types(data):\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            data[k] = convert_values_to_types(v)\n",
    "        return data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        if data:\n",
    "            return f'[{convert_values_to_types(data[0])}]'\n",
    "        return []\n",
    "\n",
    "    if isinstance(data, str) and len(data) == 32:\n",
    "        return 'uuid'\n",
    "\n",
    "    return data.__class__.__name__   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b35ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary in the yaml like format to make it readible\n",
    "\n",
    "def save_formatted(obj, file, padding = ''):\n",
    "    if isinstance(obj, list):\n",
    "        for i in obj:\n",
    "            save_formatted(i, file, padding)\n",
    "\n",
    "        \n",
    "    elif isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            file.write(f'{padding}{k}:\\n')\n",
    "            save_formatted(v, file, padding + '\\t')\n",
    "            if not padding:\n",
    "                file.write('\\n')\n",
    "    \n",
    "    else:\n",
    "        file.write(f'{padding}{obj}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f453eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save structure of fixture to the file\n",
    "\n",
    "def save_structure(data, suffix='', keep_values=False):\n",
    "    data = group_by_model(data)\n",
    "    data = compress_groups(data)\n",
    "    if not keep_values:\n",
    "        data = {k: convert_values_to_types(v) for k,v in data.items()}\n",
    "    \n",
    "    with open(f'structure{suffix}.txt', 'w') as f:\n",
    "        save_formatted(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f800a2",
   "metadata": {},
   "source": [
    "## Main script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0c05c",
   "metadata": {},
   "source": [
    "### Fixture1.5 - tiding up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f0ac4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture.json.gz')\n",
    "data = [i for i in data if i['model'].split('.')[0] == 'cedamoles_app']\n",
    "dict_to_json(data, 'fixture1.5.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ec823",
   "metadata": {},
   "source": [
    "### Fixture2 - flat fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a836329-7f92-464a-b58c-925644e5db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after all the interation fixture is flatten down to the 11 refereanceable models and saved as fixture2.json\n",
    "\n",
    "data = json_to_dict('fixture1.5.json.gz')\n",
    "data = include_referenceable(data)\n",
    "data = include_simple_field(data, 'discoveryserviceid', [('observation', 'discoveryKeywords'), ('observationcollection', 'discoveryKeywords')])\n",
    "data = include_simple_field(data, 'dqconformanceresult', [('observation', 'resultQuality')])\n",
    "data = include_simple_field(data, 'constraints', [('imagedetails', 'imageConstraints'),('observation', 'permission')])\n",
    "data = include_simple_field(data, 'imagedetails', [('acquisition', 'imageDetails'), \n",
    "                                                  ('computation', 'imageDetails'),\n",
    "                                                  ('instrument', 'imageDetails'),\n",
    "                                                  ('observationcollection', 'imageDetails'),\n",
    "                                                  ('observation', 'imageDetails'),\n",
    "                                                  ('platform', 'imageDetails'),\n",
    "                                                  ('project', 'imageDetails'),\n",
    "                                                  ])\n",
    "data = include_simple_field(data, 'vocabularyterm', [('observation', 'vocabularyKeywords')])\n",
    "data = include_simple_field(data, 'verticalextent', [('observation', 'verticalExtent')])\n",
    "data = include_simple_field(data, 'timeperiod', [('mobileplatformoperation', 'operationTime'),\n",
    "                                                 ('observation', 'timePeriod'),\n",
    "                                                 ('observation', 'validTimePeriod')])\n",
    "data = include_simple_field(data, 'party', [('responsiblepartyinfo', 'party'),\n",
    "                                            ('review', 'commentator'),\n",
    "                                            ('review', 'reviewer'),\n",
    "                                            ('note', 'commentator')])\n",
    "data = include_simple_field(data, 'phenomenonname', [('phenomenon', 'names')])\n",
    "data = include_simple_field(data, 'phenomenonterm', [('phenomenon', 'terms')])\n",
    "data = include_simple_field(data, 'phenomenon', [('observation', 'phenomena')])\n",
    "data = include_simple_field(data, 'geographicboundingbox', [('mobileplatformoperation', 'location'),\n",
    "                                                            ('observation', 'geographicExtent'),\n",
    "                                                            ('platform', 'location')])\n",
    "\n",
    "data = include_on_foreign_key(data, 'drsdataset', 'drsDataset')\n",
    "data = include_on_foreign_key(data, 'identifier')\n",
    "data = include_on_foreign_key(data, 'onlineresource', 'onlineResource')\n",
    "data = include_on_foreign_key(data, 'migrationproperty', 'migrationProperty')\n",
    "data = include_on_foreign_key(data, 'note')\n",
    "data = include_on_foreign_key(data, 'responsiblepartyinfo', 'responsiblePartyInfo')\n",
    "data = include_on_foreign_key(data, 'review')\n",
    "\n",
    "data = include_simple_field(data, 'inputoutputdescription', [('computation', 'inputDescription'),\n",
    "                                                            ('computation', 'outputDescription'),\n",
    "                                                            ('acquisition', 'outputDescription')])\n",
    "data = include_on_foreign_key(data, 'instrumentplatformpair', 'instrumentPlatformPair')\n",
    "data = include_simple_field(data, 'reviewnote', [('review', 'reviewNotes')])\n",
    "\n",
    "\n",
    "\n",
    "data = include_related_obs_info(data)\n",
    "\n",
    "dict_to_json(data, 'fixture2.json.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867624c6",
   "metadata": {},
   "source": [
    "### Fixture3 - relations via UUID; PKs removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baa32a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relations between models are established by UUIDs; PKs got removed; fixture saved as fixture3.json\n",
    "\n",
    "data = json_to_dict('fixture2.json.gz')\n",
    "data = swap_pks_to_uuids(data)\n",
    "data = remove_pks(data)\n",
    "dict_to_json(data, 'fixture3.json.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9ee8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture3.json.gz')\n",
    "save_structure(data, '_3')\n",
    "save_distribution_to_the_file(data, 'distribution_3.txt')\n",
    "save_distribution_to_the_file(data, 'distribution_3.txt', True)\n",
    "save_distribution_to_the_file(data, 'distribution_3_m.txt', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af4ecb",
   "metadata": {},
   "source": [
    "## Fixture3.5 - modifying geo extent to match the ES format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16cca06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_geo_extent(geo_extent):\n",
    "    if geo_extent is None:\n",
    "        return None\n",
    "        \n",
    "    output = {'type' : 'envelope',\n",
    "            'coordinates': []}\n",
    "    \n",
    "    output['coordinates'] = [\n",
    "        [geo_extent['westBoundLongitude'], \n",
    "        geo_extent['northBoundLatitude']], \n",
    "        [geo_extent['eastBoundLongitude'],\n",
    "        geo_extent['southBoundLatitude']]]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f56b8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture3.json.gz')\n",
    "\n",
    "for x, i in enumerate(data):\n",
    "        if i['model'] == 'observation':\n",
    "            i['geographicExtent'] = modify_geo_extent(i['geographicExtent'])\n",
    "            data[x] = i\n",
    "        \n",
    "        if i['model'] in ['mobileplatformoperation', 'platform']:\n",
    "            if 'location' in i:\n",
    "                i['geographicExtent'] = modify_geo_extent(i['location'])\n",
    "                i = {k: v for k, v in i.items() if k != 'location'}\n",
    "                data[x] = i\n",
    "\n",
    "\n",
    "dict_to_json(data, 'fixture3.5.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34014489",
   "metadata": {},
   "source": [
    "### Fixture4 - nulls removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbbbbfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields which haven't been used across the entire DB are removed from it; fixture saved as fixture4.json\n",
    "\n",
    "data = json_to_dict('fixture3.json.gz')\n",
    "grouped_data = group_by_model(data)\n",
    "distribution = count_not_nulls(grouped_data)\n",
    "distribution = {k: v for k, v in distribution.items() if v == 0}\n",
    "\n",
    "fields_to_be_removed = []\n",
    "with open('empty_fields.txt', 'w') as f:\n",
    "    for i in distribution:\n",
    "        f.write(f'{i}\\n')\n",
    "        sep = i.find('/')\n",
    "        model = i[:sep]\n",
    "        path = i[sep + 1:]\n",
    "        fields_to_be_removed.append((model, path))\n",
    "\n",
    "fields_to_be_removed\n",
    "remove_fields_from_models_in_fixture(fields_to_be_removed, 'fixture3.json.gz', 'fixture4.json.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8b597e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = json_to_dict('fixture4.json.gz')\n",
    "save_structure(data, '_4')\n",
    "save_distribution_to_the_file(data, 'distribution_4.txt')\n",
    "save_distribution_to_the_file(data, 'distribution_4_grouped.txt', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ade929",
   "metadata": {},
   "source": [
    "### Fixture5 - irrelevant fields removed after the discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94feb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any fields specified in the fields_to_be_removed are be removed from the fixture and new fixture is saved as fixture5.json\n",
    "\n",
    "fields_to_be_removed = [\n",
    "    ('result', 'review'),\n",
    "]\n",
    "\n",
    "remove_fields_from_models_in_fixture(fields_to_be_removed, 'fixture4.json.gz', 'fixture5.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ab752",
   "metadata": {},
   "source": [
    "## Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f58cb",
   "metadata": {},
   "source": [
    "### Analyzing sizes of fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "df2891af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements(data):\n",
    "    if not isinstance(data, dict) and not isinstance(data, list):\n",
    "        return 0\n",
    "    \n",
    "    result = 0\n",
    "    if isinstance(data, list):\n",
    "        for i in data:\n",
    "            result += count_elements(i)\n",
    "        return result\n",
    "    \n",
    "    for k, v in data.items():\n",
    "        result += 1\n",
    "        result += count_elements(v)\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f5c4bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def analyze_fixtures(filename):\n",
    "    suffixes = ['1.5', '2', '3', '3.5',]\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('name\\tsize[MB]\\tnumber_of_models\\tnumber_of_fields\\n')\n",
    "        for s in suffixes:\n",
    "            fname = f'fixture{s}.json.gz'\n",
    "            data = json_to_dict(fname)\n",
    "            fsize = Path(fname).stat().st_size / 1000000\n",
    "            num_of_models = len(data)\n",
    "            num_of_fields = count_elements(data)\n",
    "            f.write(f'{fname}\\t{fsize}\\t{num_of_models}\\t{num_of_fields}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d36a3",
   "metadata": {},
   "source": [
    "### Analyzing deepness of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bda710da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_of_deepness(filein, fileout):\n",
    "    data = json_to_dict(filein)\n",
    "    data = group_by_model(data)\n",
    "    data = count_not_nulls(data)   \n",
    "    \n",
    "    output = dict()\n",
    "    for k in data:\n",
    "        k1 = len(k.split('/')) - 1\n",
    "        if k1 in output:\n",
    "            output[k1].append(k)\n",
    "        else:\n",
    "            output[k1] = [k] \n",
    "    \n",
    "    with open(fileout, 'w') as f:\n",
    "        save_formatted(output, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc21af8",
   "metadata": {},
   "source": [
    "### Unzipping fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11dbff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_fixture(filename):\n",
    "    data = json_to_dict(filename)\n",
    "\n",
    "    newname = filename.split('.')\n",
    "    newname = \".\".join(newname[:-1])\n",
    "\n",
    "    with open(newname, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34590b8",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "96a2caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_datetime(value):\n",
    "    if not value or not isinstance(value, str):\n",
    "        return False\n",
    "  \n",
    "    try:\n",
    "        if value[4] == '-' and value[7] == '-' and value[10] == 'T':\n",
    "            return True\n",
    "    except IndexError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5e0aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_fields(data, path=''):\n",
    "    output = dict()\n",
    "\n",
    "    if not data:\n",
    "        return output\n",
    "\n",
    "    if isinstance(data, str) and is_datetime(data):\n",
    "        output[path] = data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        for i in data:\n",
    "            output.update(get_date_fields(i, path))\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            output.update(get_date_fields(v, f'{path}/{k}'))\n",
    "    \n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "947fddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_attributes(data, number_of_appearance, filename):\n",
    "    distribution = group_by_model(data)\n",
    "    distribution = count_not_nulls(distribution, False)\n",
    "\n",
    "    less_than_x = [k for k, v in distribution.items() if v < number_of_appearance]\n",
    "    less_than_x = [i.split('/')[1:] for i in less_than_x]   \n",
    "\n",
    "    output = {'/'.join(k): [] for k in less_than_x}\n",
    "\n",
    "    for i in data:\n",
    "        for p in less_than_x:\n",
    "            field = i\n",
    "            for f in p:\n",
    "                if field is not None and f in field:\n",
    "                    field = field[f]\n",
    "                else:\n",
    "                    field = None\n",
    "                    break\n",
    "\n",
    "            if field is not None and field:\n",
    "                output['/'.join(p)].append(f\"{i['uuid']} {i['model']}\")\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        save_formatted(output, f)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b1748f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture3.5.json.gz')\n",
    "rare_fields = get_rare_attributes(data, 15, 'rare_fields_with_uuids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "25063129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_by_uuid(data, uuid):\n",
    "    for i in data:\n",
    "        if i['uuid'] == uuid:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "344fa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_uuids(rare_fields, fieldname):\n",
    "    return [i.split()[0] for i in rare_fields[fieldname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture3.5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "82c643c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = get_rare_uuids(rare_fields, 'review/reviewer/phone')\n",
    "\n",
    "items = []\n",
    "for u in uuids:\n",
    "    items.append(get_by_uuid(data, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0cd55981",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_pick = ['review', 'uuid']\n",
    "\n",
    "items_filtered = [{k: v for k, v in i.items() if k in fields_to_pick} for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "217970e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'HADRT2.1: Bias adjusted global monthly fields of radiosonde temperature anomalies (1958-2004)',\n",
       " 'abstract': 'The HADRT2.1 data are global monthly fields of radiosonde temperature anomalies at standard pressure levels on a 5 degree latitude by 10 degree longitude grid from 1958 to July 2004. \\r\\nAnomalies are calculated with respect to 1971-1990 climatology. Anomalies are available for 9 standard levels (850, 700, 500, 300, 200, 150, 100, 50, 30hPa) as well as tropospheric (850 - 300hPa) and stratospheric (150 - 30hPa) averages.\\r\\nThe data are degree Celsius anomalies from 1970-1990 means. Anomalies are calculated for each of about 200 sonde stations worldwide and grid values derived from these. \\r\\n\\r\\nHADRT2.1 is as HadRT2.0 but with bias corrections made to many station time series world-wide. The adjustments were calculated by reference to MSU data products, but only for known changes in instrumental or operational procedures for the period post 1979. No data are available for the 30hPa level in this version. \\r\\n\\r\\nThis dataset has been superseded by the HadAT dataset also available from CEDA.',\n",
       " 'submissionUserID': None,\n",
       " 'creationDate': '2022-07-22T09:15:57.183Z',\n",
       " 'lastUpdatedDate': '2022-07-22T09:15:57.272Z',\n",
       " 'latestDataUpdateTime': None,\n",
       " 'updateFrequency': 'notPlanned',\n",
       " 'dataLineage': 'Data obtained for the period from 1958 to July 2004, were provided as is by the Met Office Hadley Centre, for archiving at the BADC.',\n",
       " 'removedDataReason': '',\n",
       " 'keywords': 'Met Office, HADRT, radiosonde, temperature anomalies, Hadley',\n",
       " 'publicationState': 'published',\n",
       " 'nonGeographicFlag': False,\n",
       " 'dontHarvestFromProjects': False,\n",
       " 'geographicExtent': {'type': 'envelope',\n",
       "  'coordinates': [[-180.0, 90.0], [180.0, -90.0]]},\n",
       " 'language': 'English',\n",
       " 'permission': {'useLimitation': None,\n",
       "  'accessConstraints': None,\n",
       "  'accessCategory': 'restricted',\n",
       "  'accessRoles': 'hadrt',\n",
       "  'licenceURL': 'http://licences.ceda.ac.uk/image/data_access_condition/ukmo_agreement.pdf',\n",
       "  'label': None},\n",
       " 'resolution': '5x10 grid',\n",
       " 'status': 'obsolete',\n",
       " 'verticalExtent': None,\n",
       " 'result_field': '6fd09834e8ed48f4a9e3ab4170226f30',\n",
       " 'timePeriod': {'startTime': '1958-01-01T00:00:00Z',\n",
       "  'endTime': '2004-07-30T23:00:00Z'},\n",
       " 'resultQuality': {'explanation': 'Data quality controlled by the Met Office. Please see main reference paper for this dataset.',\n",
       "  'passesTest': True,\n",
       "  'resultTitle': 'CEDA Data Quality Statement',\n",
       "  'date': '2014-06-09'},\n",
       " 'dataPublishedTime': '2015-12-19T18:12:30Z',\n",
       " 'doiPublishedTime': None,\n",
       " 'removedDataTime': None,\n",
       " 'validTimePeriod': None,\n",
       " 'procedureDescription': '',\n",
       " 'procedureAcquisition': None,\n",
       " 'procedureComputation': None,\n",
       " 'procedureCompositeProcess': 'db186b9ce9804ab8a39be21087c7a938',\n",
       " 'featureOfInterest': '',\n",
       " 'imageDetails': [{'linkage': 'http://artefacts.ceda.ac.uk/graphics/logos/Hadley_Centre.jpg',\n",
       "   'fileName': 'Met Office Hadley Centre (MOHC) Logo',\n",
       "   'fileDescription': 'Met Office Hadley Centre Logo',\n",
       "   'imageConstraints': None}],\n",
       " 'discoveryKeywords': [{'name': 'NDGO0003'}],\n",
       " 'externalCitations': [],\n",
       " 'projects': ['ce252c81a7bd4717834055e31716b265'],\n",
       " 'inputParameter': [],\n",
       " 'inspireTheme': [],\n",
       " 'topicCategory': [],\n",
       " 'phenomena': [{'names': [{'name': 'Vertical Profile Of Air Temperature'}],\n",
       "   'terms': [{'label': 'long_name',\n",
       "     'value': 'Vertical Profile Of Air Temperature',\n",
       "     'vocabulary': ''}]},\n",
       "  {'names': [{'name': 'Air Temperature'},\n",
       "    {'name': 'http://vocab.ndg.nerc.ac.uk/term/P141/4/GVAR0027'}],\n",
       "   'terms': [{'label': 'long_name',\n",
       "     'value': 'Air Temperature',\n",
       "     'vocabulary': ''},\n",
       "    {'label': 'gcmd_url',\n",
       "     'value': 'http://vocab.ndg.nerc.ac.uk/term/P141/4/GVAR0027',\n",
       "     'vocabulary': ''},\n",
       "    {'label': 'gcmd_keyword',\n",
       "     'value': 'Air Temperature',\n",
       "     'vocabulary': 'gcmd_keyword'}]},\n",
       "  {'names': [{'name': 'http://vocab.ndg.nerc.ac.uk/term/P141/4/GVAR0861'},\n",
       "    {'name': 'Temperature Anomalies'}],\n",
       "   'terms': [{'label': 'long_name',\n",
       "     'value': 'Temperature Anomalies',\n",
       "     'vocabulary': ''},\n",
       "    {'label': 'gcmd_url',\n",
       "     'value': 'http://vocab.ndg.nerc.ac.uk/term/P141/4/GVAR0861',\n",
       "     'vocabulary': ''},\n",
       "    {'label': 'gcmd_keyword',\n",
       "     'value': 'Temperature Anomalies',\n",
       "     'vocabulary': 'gcmd_keyword'}]}],\n",
       " 'vocabularyKeywords': [],\n",
       " 'uuid': '3ab1423a6b8d43afa7199b0331e1d65f',\n",
       " 'identifier': {'url': 'Hadley-HADRT2.1',\n",
       "  'identifierType': 'ceda_abbreviation',\n",
       "  'shortUrl': ''},\n",
       " 'onlineResource': [{'applicationProfile': None,\n",
       "   'function': 'documentation',\n",
       "   'internalResourceType': None,\n",
       "   'linkage': 'http://dx.doi.org/10.1029/97GL01186',\n",
       "   'name': 'Parker, D.E. et al., 1997: A new global gridded radiosonde temperature data base and recent temperature trends, GRL, 24, 12',\n",
       "   'description': None},\n",
       "  {'applicationProfile': None,\n",
       "   'function': 'documentation',\n",
       "   'internalResourceType': None,\n",
       "   'linkage': 'http://data.ceda.ac.uk/badc/ukmo-hadrt/doc/Read_instruction_hadrt2.txt',\n",
       "   'name': 'Met Office Hadley Centre HadRT documentation',\n",
       "   'description': None},\n",
       "  {'applicationProfile': None,\n",
       "   'function': 'documentation',\n",
       "   'internalResourceType': None,\n",
       "   'linkage': 'http://data.ceda.ac.uk/badc/ukmo-hadrt/doc/Read_instruction_hadrt2.txt',\n",
       "   'name': 'Met Office HadRT2.1 data format information',\n",
       "   'description': None}],\n",
       " 'note': {'comments': 'The HadRT dataset was found to be inadequate as a climate dataset and therefore the Met Office produced the HadAT dataset which supersedes HadRT data.  Please note that it is a term of usage of this dataset that it is used in conjunction with our HadAT dataset.',\n",
       "  'commentator': 'f5f575a13d5258c7e6521fe4fe69c916',\n",
       "  'date': '2007-03-06T14:37:59Z'},\n",
       " 'responsiblePartyInfo': [{'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'publisher'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'Hadley Centre for Climate Prediction and Research (MOHC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': '',\n",
       "    'deliveryPoint': 'Met Office',\n",
       "    'administrativeArea': '',\n",
       "    'city': 'Exeter',\n",
       "    'country': '',\n",
       "    'postalCode': '',\n",
       "    'electronicEmailAddress': '',\n",
       "    'phone': '',\n",
       "    'onlineResource': ''},\n",
       "   'role': 'author'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'custodian'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'distributor'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'point_of_contact'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'metadata_owner'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': '',\n",
       "    'lastName': 'NCAS British Atmospheric Data Centre (NCAS BADC)',\n",
       "    'partyType': 'organisation',\n",
       "    'description': \"The British Atmospheric Data Centre (BADC) is the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences. The role of the BADC is to assist UK atmospheric researchers to locate, access and interpret atmospheric data and to ensure the long-term integrity of atmospheric data produced by NERC projects. The BADC is run by he Centre for Environmental Data Archival (CEDA).\",\n",
       "    'deliveryPoint': 'British Atmospheric Data Centre',\n",
       "    'administrativeArea': 'STFC Rutherford Appleton Laboratory',\n",
       "    'city': 'Harwell Oxford',\n",
       "    'country': 'UK',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'badc@rl.ac.uk',\n",
       "    'phone': '+44(0)1235 446432',\n",
       "    'onlineResource': 'http://badc.rl.ac.uk/'},\n",
       "   'role': 'curator'},\n",
       "  {'priority': 1,\n",
       "   'party': {'firstName': 'Kevin',\n",
       "    'lastName': 'Marsh',\n",
       "    'partyType': 'individual',\n",
       "    'description': '',\n",
       "    'deliveryPoint': '',\n",
       "    'administrativeArea': '',\n",
       "    'city': '',\n",
       "    'country': '',\n",
       "    'postalCode': '',\n",
       "    'electronicEmailAddress': 'kevin.marsh@stfc.ac.uk',\n",
       "    'phone': '',\n",
       "    'onlineResource': ''},\n",
       "   'role': 'ceda_officer'},\n",
       "  {'priority': 2,\n",
       "   'party': {'firstName': 'Anabelle',\n",
       "    'lastName': 'Guillory',\n",
       "    'partyType': 'individual',\n",
       "    'description': '',\n",
       "    'deliveryPoint': '',\n",
       "    'administrativeArea': '',\n",
       "    'city': '',\n",
       "    'country': '',\n",
       "    'postalCode': '',\n",
       "    'electronicEmailAddress': 'anabelle.guillory@stfc.ac.uk',\n",
       "    'phone': '',\n",
       "    'onlineResource': 'http://www.ceda.ac.uk/about/team/#anabelle'},\n",
       "   'role': 'ceda_officer'},\n",
       "  {'priority': 3,\n",
       "   'party': {'firstName': 'Ruth',\n",
       "    'lastName': 'Petrie',\n",
       "    'partyType': 'individual',\n",
       "    'description': 'CEDA Officer',\n",
       "    'deliveryPoint': 'Centre for Environmental Data Analysis (CEDA)',\n",
       "    'administrativeArea': 'Science and Technologies Facilities Council',\n",
       "    'city': 'Harwell',\n",
       "    'country': 'United Kingdom',\n",
       "    'postalCode': 'OX11 0QX',\n",
       "    'electronicEmailAddress': 'ruth.petrie@stfc.ac.uk',\n",
       "    'phone': '01235445980',\n",
       "    'onlineResource': ''},\n",
       "   'role': 'ceda_officer'},\n",
       "  {'priority': 4,\n",
       "   'party': {'firstName': 'Ag',\n",
       "    'lastName': 'Stephens',\n",
       "    'partyType': 'individual',\n",
       "    'description': '',\n",
       "    'deliveryPoint': '',\n",
       "    'administrativeArea': '',\n",
       "    'city': '',\n",
       "    'country': '',\n",
       "    'postalCode': '',\n",
       "    'electronicEmailAddress': 'ag.stephens.stfc.ac.uk',\n",
       "    'phone': '',\n",
       "    'onlineResource': ''},\n",
       "   'role': 'ceda_officer'}],\n",
       " 'review': {'reviewer': {'firstName': 'Ruth',\n",
       "   'lastName': 'Petrie',\n",
       "   'partyType': 'individual',\n",
       "   'description': 'CEDA Officer',\n",
       "   'deliveryPoint': 'Centre for Environmental Data Analysis (CEDA)',\n",
       "   'administrativeArea': 'Science and Technologies Facilities Council',\n",
       "   'city': 'Harwell',\n",
       "   'country': 'United Kingdom',\n",
       "   'postalCode': 'OX11 0QX',\n",
       "   'electronicEmailAddress': 'ruth.petrie@stfc.ac.uk',\n",
       "   'phone': '01235445980',\n",
       "   'onlineResource': ''},\n",
       "  'reviewFrequency': 'yearly',\n",
       "  'reviewStatus': 'progressing',\n",
       "  'reviewNotes': []},\n",
       " 'relatedObservationInfo': {'subjectObservation': '4ae1ae0cdbc04533b5e626a811d056f7',\n",
       "  'relationType': 'IsDerivedFrom'},\n",
       " 'model': 'observation'}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "3fa628cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_dict('fixture3.5.json.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "001576a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [i['responsiblePartyInfo'] for i in data if 'responsiblePartyInfo' in i]\n",
    "counted = [(len(i) if isinstance(i, list) else 1) for i in items]\n",
    "grouped_by_count = dict()\n",
    "for i in counted:\n",
    "    grouped_by_count[i] = grouped_by_count.get(i, 0) + 1\n",
    "\n",
    "grouped_by_count = dict(sorted(grouped_by_count.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1c062c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1397,\n",
       " 2: 5238,\n",
       " 3: 1069,\n",
       " 4: 360,\n",
       " 5: 1688,\n",
       " 6: 255,\n",
       " 7: 281,\n",
       " 8: 2211,\n",
       " 9: 2596,\n",
       " 10: 1897,\n",
       " 11: 909,\n",
       " 12: 1236,\n",
       " 13: 300,\n",
       " 14: 116,\n",
       " 15: 309,\n",
       " 16: 54,\n",
       " 17: 37,\n",
       " 18: 41,\n",
       " 19: 23,\n",
       " 20: 30,\n",
       " 21: 15,\n",
       " 22: 5,\n",
       " 23: 21,\n",
       " 24: 11,\n",
       " 25: 5,\n",
       " 26: 14,\n",
       " 27: 6,\n",
       " 28: 4,\n",
       " 29: 2,\n",
       " 30: 11,\n",
       " 31: 3,\n",
       " 32: 1,\n",
       " 33: 4,\n",
       " 34: 13,\n",
       " 35: 35,\n",
       " 36: 12,\n",
       " 37: 1,\n",
       " 38: 12,\n",
       " 39: 3,\n",
       " 40: 1,\n",
       " 42: 1,\n",
       " 49: 1,\n",
       " 50: 1,\n",
       " 51: 1,\n",
       " 52: 1,\n",
       " 55: 1,\n",
       " 64: 1,\n",
       " 65: 4,\n",
       " 66: 1,\n",
       " 70: 1,\n",
       " 71: 2,\n",
       " 72: 5,\n",
       " 73: 1,\n",
       " 74: 12,\n",
       " 75: 24,\n",
       " 87: 1,\n",
       " 95: 1,\n",
       " 98: 49,\n",
       " 101: 1,\n",
       " 102: 10,\n",
       " 105: 48,\n",
       " 161: 1}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "ce5327e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6a42c000c2ad4532b74c08b16a33992d',\n",
       " 'fdf588322e3b4ba5a083a822e1b5ebbd',\n",
       " '41da8d4e2a2843049c4ec7d6b52a23c8',\n",
       " 'a66ea4a45396463fbaceb5dd4b1886b5',\n",
       " '976fa74c1f3444d19133840bcf6b320f',\n",
       " 'd6e0c02eb7d546738d6620aa05893d5d',\n",
       " '590f273c0ab14033ac50bd2f0706de8d',\n",
       " '8c135f786d904e45a0f5f7204675db73',\n",
       " '39279a787f69427d815192ca6458582e',\n",
       " '27813c7c87894902bdb672a5fc16cc1f',\n",
       " 'd2d093f4afe54cf49da199db83dbc51e',\n",
       " 'c20d07b4e692494f919382310e43b225',\n",
       " 'dc567372a8424c3097988d60db7cf06c',\n",
       " 'ef9dc7705e0648f8b6c1f23cf0ef8ed1',\n",
       " 'd1535aa7a15f43f9a03642ca26f92c51',\n",
       " '376553085b61406b91a80ccc7fcd6bb4',\n",
       " 'de71cf74fb504bb9bb8008de4aa8b95f',\n",
       " 'b7952776ba5546fdb2a15104a6f382b0',\n",
       " '14ca2574a76e4b2d843e94c5c7be6be4',\n",
       " '2fddc05e8b224385a147073a62f26d4b',\n",
       " '6d4ba43b7596451192298dd566cb1927',\n",
       " 'b91e9e2f056c4046b587fb71c5da44ba',\n",
       " '229b7a8ccb8e43839422252048eb9958',\n",
       " 'c8b198d4c94e48fa921234848b40f7f7',\n",
       " '8f791a3acdaf4308974562960e18421b',\n",
       " 'de16a3a97ce1424bb8b5c7a704692c71',\n",
       " '246db47c9e104710a43262b99f553490',\n",
       " '5e0bfd6b565c45d69bd252828e3f035b',\n",
       " 'ddeb7f738a484c378a3e3344d15c33ef',\n",
       " '03052e0327fe45d78781bcda6b1b7eb9',\n",
       " '430e6965817a4dba97e8e5509a241e37',\n",
       " '64853764956c4f50be8f4ef82a3e0785',\n",
       " '8592c7058c714d0ebd8b4613388acda1',\n",
       " 'd963abaf773846ecb4bba431f3cd7fb2',\n",
       " '7a81079b6e5746e89e540da4fc68a5b5',\n",
       " 'ea4130db1fdf4e5191e7a5ad3d311b87',\n",
       " '2edee495292548c6ad84e4a3de7043b2',\n",
       " 'a3bae854fb1644abbe8a32d398710ada',\n",
       " '4924f423bbc646c3b650cd9a92aff4bb',\n",
       " '6ae0330902f048b5967baf0d49245ad1',\n",
       " 'd50db5c348bd42da914a584c62d486b5',\n",
       " '5181cc35968d4b6f824f53dacc312818',\n",
       " 'bcd2d7f025864027ae0ae76b5b78b470',\n",
       " '9e353a18cd534b2f98b092c1003f7f6b',\n",
       " 'eeb5cf999f464f558207193851ec32e2',\n",
       " '12b1a22e184e4fba8b7ce6f25ff9e58d',\n",
       " '54cfeda48f204e56b4060486a91493da',\n",
       " 'b48e1720880c4353949c0907f460bedb']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i['uuid'] for i in data if 'responsiblePartyInfo' in i and isinstance(i['responsiblePartyInfo'], list) and len(i['responsiblePartyInfo']) == 105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "d79ce4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_by_uuid(data, '42a4ab662c734acfa46d5a1b231364ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "ca2245db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " list,\n",
       " dict,\n",
       " dict,\n",
       " list,\n",
       " ...]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(i) for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
